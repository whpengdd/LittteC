"""
æ‰¹é‡åˆ†ææœåŠ¡ - åå°æ‰¹é‡ LLM åˆ†ææ ¸å¿ƒé€»è¾‘

æä¾›å¹¶è¡Œå¤„ç†ã€å¤±è´¥é‡è¯•å’Œè¿›åº¦è·Ÿè¸ªåŠŸèƒ½ã€‚
"""
import asyncio
import uuid
from typing import Dict, List, Optional, Any
from datetime import datetime
import json

from services.db_service import get_db_service
from services.config_service import get_config_service


# é»˜è®¤åˆ†æ Prompt æ¨¡æ¿ï¼ˆæ¶‰å¯†/åˆè§„åˆ†æ + æ ‡ç­¾æå–ï¼‰
DEFAULT_ANALYSIS_PROMPT = """åŸºäºä»¥ä¸‹é‚®ä»¶å¾€æ¥ï¼Œä»¥ JSON æ ¼å¼è¿”å›åˆ†æç»“æœï¼š
{
    "risk_level": "ä½/ä¸­/é«˜",
    "summary": "100å­—ä»¥å†…çš„æ ¸å¿ƒå†…å®¹ç®€è¿°",
    "tags": ["æ ‡ç­¾1", "æ ‡ç­¾2", "æ ‡ç­¾3"],
    "key_findings": "å¦‚æœ‰æ•æ„Ÿæˆ–åˆè§„ç›¸å…³å†…å®¹ï¼Œè¯·è¯´æ˜ï¼›å¦åˆ™ç•™ç©º"
}

é‚®ä»¶å†…å®¹ï¼š
{content}

è¯·åªè¾“å‡º JSONï¼Œä¸è¦æœ‰ä»»ä½•å‰ç¼€æˆ–è§£é‡Šã€‚æ‰€æœ‰å­—æ®µå€¼å¿…é¡»ä½¿ç”¨**ç®€ä½“ä¸­æ–‡**ã€‚risk_level å¿…é¡»æ˜¯ "é«˜"ã€"ä¸­"ã€"ä½" ä¹‹ä¸€ã€‚"""

# é»˜è®¤è¿‡æ»¤å…³é”®è¯
DEFAULT_FILTER_KEYWORDS = [
    "Systems bounce",
    "Verify",
    "Auto-Reply",
    "Out of Office",
    "Delivery Status",
    "Undeliverable"
]

# æ­£åœ¨è¿è¡Œçš„ä»»åŠ¡å­˜å‚¨
_running_jobs: Dict[str, asyncio.Task] = {}


class BatchAnalysisService:
    """æ‰¹é‡åˆ†ææœåŠ¡"""
    
    def __init__(self):
        self.db = get_db_service()
        # ä»»åŠ¡çº§åˆ«çš„è„±æ•æœåŠ¡å®ä¾‹ç®¡ç†ï¼ˆç¡®ä¿åŒä¸€ä»»åŠ¡ä¸­ Token ä¸€è‡´ï¼‰
        self.task_masking_services = {}  # task_id -> PIIMaskingService
        # æœåŠ¡å¯åŠ¨æ—¶æ¸…ç†åƒµå°¸ä»»åŠ¡
        self._cleanup_zombie_jobs()
    
    async def create_and_start_job(
        self,
        task_id: str,
        prompt: str = None,
        filter_keywords: List[str] = None,
        model: str = None,
        concurrency: int = 5,
        max_retries: int = 3,
        analysis_type: str = "email"
    ) -> Dict[str, Any]:
        """
        åˆ›å»ºå¹¶å¯åŠ¨æ‰¹é‡åˆ†æä»»åŠ¡
        
        Args:
            task_id: é‚®ä»¶ä»»åŠ¡ ID
            prompt: åˆ†æ Promptï¼Œä¸ºç©ºåˆ™ä½¿ç”¨é»˜è®¤æ¨¡æ¿
            filter_keywords: è¿‡æ»¤å…³é”®è¯åˆ—è¡¨
            model: AI æ¨¡å‹ (gemini/azure)
            concurrency: å¹¶è¡Œåº¦
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
            analysis_type: åˆ†æç±»å‹ ("email", "people_cluster", "subject_cluster")
        
        Returns:
            ä»»åŠ¡è¯¦æƒ…
        """
        job_id = str(uuid.uuid4())
        
        # ä½¿ç”¨é»˜è®¤å€¼
        if prompt is None:
            prompt = DEFAULT_ANALYSIS_PROMPT
        if filter_keywords is None:
            filter_keywords = DEFAULT_FILTER_KEYWORDS
        if model is None:
            model = get_config_service().get_llm_provider()
        
        # åˆ›å»ºä»»åŠ¡è®°å½•
        job = self.db.create_batch_job(
            job_id=job_id,
            task_id=task_id,
            prompt=prompt,
            filter_keywords=filter_keywords,
            model_provider=model,
            concurrency=concurrency,
            max_retries=max_retries,
            analysis_type=analysis_type
        )
        
        # åœ¨åå°å¯åŠ¨ä»»åŠ¡
        task = asyncio.create_task(self._run_job(job_id))
        _running_jobs[job_id] = task
        
        return job
    
    async def resume_job(self, old_job_id: str) -> Dict[str, Any]:
        """
        æ¢å¤å·²ä¸­æ–­æˆ–å–æ¶ˆçš„ä»»åŠ¡
        æœ¬è´¨æ˜¯åˆ›å»ºä¸€ä¸ªæ–°ä»»åŠ¡ï¼Œä½†ä½¿ç”¨æ—§ä»»åŠ¡çš„é…ç½®
        ç”±äº analyze process ä¼šæ£€æŸ¥æ˜¯å¦å·²åˆ†æï¼Œæ‰€ä»¥ä¼šè‡ªåŠ¨è·³è¿‡å·²å®Œæˆçš„
        """
        old_job = self.db.get_batch_job(old_job_id)
        if not old_job:
            raise ValueError("Job not found")
            
        # ä½¿ç”¨æ—§é…ç½®åˆ›å»ºæ–°ä»»åŠ¡
        return await self.create_and_start_job(
            task_id=old_job["task_id"],
            prompt=old_job["prompt"],
            filter_keywords=old_job["filter_keywords"],
            model=old_job["model_provider"],
            concurrency=old_job["concurrency"],
            max_retries=old_job["max_retries"],
            analysis_type=old_job.get("analysis_type", "email")
        )
    
    async def _run_job(self, job_id: str):
        """æ‰§è¡Œæ‰¹é‡åˆ†æä»»åŠ¡ï¼ˆåå°è¿è¡Œï¼‰"""
        db = get_db_service()
        
        try:
            # è·å–ä»»åŠ¡è¯¦æƒ…
            job = db.get_batch_job(job_id)
            if not job:
                print(f"[BatchAnalysis] Job {job_id} not found")
                return
            
            print(f"[BatchAnalysis] Starting job {job_id} with concurrency {job['concurrency']}")
            
            # æ›´æ–°çŠ¶æ€ä¸ºè¿è¡Œä¸­
            db.update_batch_job_status(job_id, "RUNNING")
            
            analysis_type = job.get("analysis_type", "email")
            
            if analysis_type == "email":
                # === é‚®ä»¶åˆ†æé€»è¾‘ ===
                emails, skipped_count = db.get_emails_for_batch_analysis(
                    job["task_id"],
                    job.get("filter_keywords", [])
                )
                items_to_process = emails
            else:
                # === èšç±»åˆ†æé€»è¾‘ ===
                # è§£æ cluster_type: "people_cluster" -> "people", "subject_cluster" -> "subjects"
                cluster_type = "people" if analysis_type == "people_cluster" else "subjects"
                clusters = db.get_clusters_for_batch_analysis(job["task_id"], cluster_type)
                items_to_process = clusters
                skipped_count = 0 # èšç±»åˆ†ææš‚æ— è¿‡æ»¤é€»è¾‘

            total_count = len(items_to_process) + skipped_count
            db.update_batch_job_total_count(job_id, total_count)
            
            print(f"[BatchAnalysis] Job {job_id} ({analysis_type}): {len(items_to_process)} items to process")
            
            # åˆå§‹åŒ–è®¡æ•°å™¨
            processed = 0
            success = 0
            failed = 0
            
            # è·å– AI æœåŠ¡
            ai_service = self._get_ai_service(job["model_provider"])
            
            # å¹¶å‘æ§åˆ¶
            semaphore = asyncio.Semaphore(job["concurrency"])
            
            async def process_item(item):
                async with semaphore:
                    try:
                        if analysis_type == "email":
                            # === é‚®ä»¶å¤„ç† ===
                            email = item
                            # æ£€æŸ¥æ˜¯å¦å·²æœ‰åˆ†æç»“æœ
                            if db.has_email_analysis(email["id"], "batch_summary"):
                                print(f"[BatchAnalysis] Email {email['id']}: Already analyzed, skipping")
                                return "EXISTING"
                            
                            print(f"[BatchAnalysis] Processing email {email['id']}")
                            
                            # æ‰§è¡Œåˆ†æï¼ˆå¸¦é‡è¯•ï¼‰
                            result = await self._analyze_with_retry(
                                ai_service,
                                email,
                                job["prompt"],
                                job["max_retries"],
                                task_id=job["task_id"]  # ä¼ é€’ task_id ç¡®ä¿è„±æ• Token ä¸€è‡´æ€§
                            )
                            
                            # ä¿å­˜ç»“æœ
                            if result:
                                analysis_id = str(uuid.uuid4())
                                db.save_analysis_result(
                                    result_id=analysis_id,
                                    task_id=job["task_id"],
                                    email_id=email["id"],
                                    analysis_type="batch_summary",
                                    model_provider=job["model_provider"],
                                    result=result
                                )
                                print(f"[BatchAnalysis] Email {email['id']}: Success")
                                return "SUCCESS"
                            else:
                                print(f"[BatchAnalysis] Email {email['id']}: Failed (no result)")
                                return "FAILED"

                        else:
                            # === èšç±»å¤„ç† ===
                            cluster = item
                            cluster_key = cluster["key"]
                            
                            # æ£€æŸ¥æ˜¯å¦å·²æœ‰åˆ†æç»“æœ (å¯é€‰ï¼Œç›®å‰èšç±»åˆ†ææ€»æ˜¯å…è®¸è¦†ç›–æ›´æ–°ï¼Œæˆ–è€…æˆ‘ä»¬å¯ä»¥æ£€æŸ¥ updated_at)
                            # è¿™é‡Œæš‚ä¸è·³è¿‡ï¼Œå› ä¸ºèšç±»å†…å®¹å¯èƒ½å˜åŒ–
                            
                            print(f"[BatchAnalysis] Processing cluster {cluster_key}")
                            
                            # æ‰§è¡Œåˆ†æ
                            cluster_type_short = "people" if analysis_type == "people_cluster" else "subjects"
                            
                             # è·å–èšç±»é‚®ä»¶ (limit 20)
                            if cluster_type_short == "people":
                                parts = cluster_key.split(" â†” ")
                                if len(parts) == 2:
                                    emails = db.get_emails_by_participants(job["task_id"], parts[0], parts[1], limit=20)
                                else:
                                    emails = []
                            else:
                                emails = db.get_emails_by_subject(job["task_id"], cluster_key, limit=20)
                            
                            if not emails:
                                return "FAILED"

                            # æ‰§è¡Œåˆ†æï¼ˆå¸¦é‡è¯•ï¼‰
                            result = await self._analyze_cluster_with_retry(
                                ai_service,
                                emails,
                                job["prompt"],  # å¯ä»¥åœ¨è¿™é‡Œæ ¹æ® analysis_type è°ƒæ•´é»˜è®¤ prompt
                                job["max_retries"],
                                task_id=job["task_id"]  # ä¼ é€’ task_id ç¡®ä¿è„±æ• Token ä¸€è‡´æ€§
                            )
                            
                            if result:
                                db.save_cluster_insight(
                                    task_id=job["task_id"],
                                    cluster_type=cluster_type_short,
                                    cluster_key=cluster_key,
                                    ai_insight=result,
                                    model=job["model_provider"]
                                )
                                print(f"[BatchAnalysis] Cluster {cluster_key}: Success")
                                return "SUCCESS"
                            else:
                                return "FAILED"

                    except Exception as e:
                        print(f"[BatchAnalysis] Error processing item: {e}")
                        return "FAILED"

            # åˆ›å»ºå¹¶æ‰§è¡Œæ‰€æœ‰ä»»åŠ¡
            tasks = [process_item(item) for item in items_to_process]
            
            if tasks:
                for future in asyncio.as_completed(tasks):
                    status = await future
                    
                    if status == "SUCCESS":
                        success += 1
                    elif status == "FAILED":
                        failed += 1
                    elif status == "EXISTING":
                        success += 1
                        
                    processed += 1
                    
                    # å®æ—¶æ›´æ–°è¿›åº¦
                    db.update_batch_job_progress(
                        job_id, processed, success, failed, skipped_count
                    )
            
            # æ›´æ–°çŠ¶æ€ä¸ºå®Œæˆ
            db.update_batch_job_status(job_id, "COMPLETED")
            print(f"[BatchAnalysis] Job {job_id} completed: {success} success, {failed} failed")
            
        except asyncio.CancelledError:
             print(f"[BatchAnalysis] Job {job_id} cancelled")
             db.update_batch_job_status(job_id, "CANCELLED")
             # ä¸éœ€è¦ re-raiseï¼Œå¦åˆ™å¤–å±‚ä¼šæŠ¥é”™ï¼Œè¿™é‡Œå·²ç»å¤„ç†äº†çŠ¶æ€
             
        except Exception as e:
            print(f"[BatchAnalysis] Job {job_id} failed: {e}")
            db.update_batch_job_status(job_id, "FAILED", str(e))
        
        finally:
            # æ¸…ç†ä»»åŠ¡å¼•ç”¨
            if job_id in _running_jobs:
                del _running_jobs[job_id]
    
    async def _analyze_with_retry(
        self,
        ai_service,
        email: Dict[str, Any],
        prompt_template: str,
        max_retries: int,
        task_id: str = None
    ) -> Optional[Dict[str, Any]]:
        """å¸¦é‡è¯•çš„å•å°é‚®ä»¶åˆ†æ"""
        from services.pii_masking_service import PIIMaskingService
        
        # è·å–æˆ–åˆ›å»ºä»»åŠ¡çº§åˆ«çš„è„±æ•æœåŠ¡å®ä¾‹
        if task_id and task_id not in self.task_masking_services:
            self.task_masking_services[task_id] = PIIMaskingService()
        masking_service = self.task_masking_services.get(task_id) if task_id else PIIMaskingService()
        
        # æ„å»ºåˆ†ææ–‡æœ¬
        raw_text = f"ä¸»é¢˜: {email.get('subject', 'æ— ä¸»é¢˜')}\n\n{email.get('content', '')}"
        
        # ğŸ”’ è„±æ•å¤„ç†ï¼šå°†æ•æ„Ÿä¿¡æ¯æ›¿æ¢ä¸º Token
        masked_text, token_map = masking_service.mask_text(raw_text)
        
        # è®°å½•è„±æ•ç»Ÿè®¡ï¼ˆè°ƒè¯•ç”¨ï¼‰
        if token_map:
            stats = masking_service.get_statistics()
            print(f"[PII] Email {email.get('id')}: è„±æ•ç»Ÿè®¡ {stats}")
        
        for attempt in range(max_retries):
            try:
                print(f"[BatchAnalysis] Email {email['id']}: Analysis attempt {attempt + 1}/{max_retries} start")
                
                # è°ƒç”¨ AI æœåŠ¡ï¼ˆå¢åŠ  60ç§’ è¶…æ—¶ä¿æŠ¤ï¼‰
                # ä½¿ç”¨ asyncio.wait_for é˜²æ­¢ API è°ƒç”¨æ— é™æŒ‚èµ·
                # âš ï¸ å…³é”®ï¼šä½¿ç”¨è„±æ•åçš„æ–‡æœ¬ï¼Œç¡®ä¿æ•æ„Ÿä¿¡æ¯ä¸æ³„éœ²ç»™ LLM
                result_model = await asyncio.wait_for(
                    ai_service.analyze_email(masked_text, prompt_template),
                    timeout=60.0
                )
                
                print(f"[BatchAnalysis] Email {email['id']}: API call success (PII masked)")

                # è½¬æ¢ä¸ºå­—å…¸
                return result_model.model_dump()
                
            except asyncio.TimeoutError:
                print(f"[BatchAnalysis] Email {email['id']}: Attempt {attempt + 1} TIMEOUT (60s)")
                if attempt < max_retries - 1:
                    await asyncio.sleep(1) # çŸ­æš‚ç­‰å¾…åé‡è¯•
                
            except Exception as e:
                print(f"[BatchAnalysis] Attempt {attempt + 1}/{max_retries} failed: {e}")
                
                if attempt < max_retries - 1:
                    # æŒ‡æ•°é€€é¿
                    wait_time = (2 ** attempt) + (0.1 * attempt)
                    await asyncio.sleep(wait_time)
                else:
                    return None
        
        return None
    
    def _get_ai_service(self, model: str):
        """è·å– AI æœåŠ¡å®ä¾‹"""
        if model == "azure":
            from services.azure_service import AzureService
            return AzureService()
        else:
            from services.gemini_service import GeminiService
            return GeminiService()
    
    def get_job_status(self, job_id: str) -> Optional[Dict[str, Any]]:
        """è·å–ä»»åŠ¡çŠ¶æ€"""
        return self.db.get_batch_job(job_id)
    
    def get_jobs_by_task(self, task_id: str) -> List[Dict[str, Any]]:
        """è·å–æŒ‡å®šä»»åŠ¡çš„æ‰€æœ‰åˆ†æä½œä¸š"""
        return self.db.get_batch_jobs_by_task(task_id)
    
    async def cancel_job(self, job_id: str) -> bool:
        """å–æ¶ˆä»»åŠ¡"""
        # æ›´æ–°æ•°æ®åº“çŠ¶æ€
        self.db.update_batch_job_status(job_id, "CANCELLED")
        
        # å°è¯•å–æ¶ˆæ­£åœ¨è¿è¡Œçš„ä»»åŠ¡
        if job_id in _running_jobs:
            task = _running_jobs[job_id]
            task.cancel()
            del _running_jobs[job_id]
            return True
        
        return False
    
    def _cleanup_zombie_jobs(self):
        """
        æ¸…ç†åƒµå°¸ä»»åŠ¡ï¼šå°†æ•°æ®åº“ä¸­çŠ¶æ€ä¸º RUNNING/PENDING ä½†å†…å­˜ä¸­ä¸å­˜åœ¨çš„ä»»åŠ¡æ ‡è®°ä¸º INTERRUPTED
        è¿™é€šå¸¸å‘ç”Ÿåœ¨æœåŠ¡é‡å¯å
        """
        try:
            # è·å–æ‰€æœ‰ä»»åŠ¡
            all_tasks = self.db.get_tasks()
            
            for task in all_tasks:
                jobs = self.db.get_batch_jobs_by_task(task["id"])
                for job in jobs:
                    if job["status"] in ("RUNNING", "PENDING") and job["id"] not in _running_jobs:
                        print(f"[BatchAnalysis] æ¸…ç†åƒµå°¸ä»»åŠ¡: {job['id']} (åŸçŠ¶æ€: {job['status']})")
                        self.db.update_batch_job_status(job["id"], "INTERRUPTED", "æœåŠ¡é‡å¯åä»»åŠ¡è¢«ä¸­æ–­")
        except Exception as e:
            print(f"[BatchAnalysis] æ¸…ç†åƒµå°¸ä»»åŠ¡æ—¶å‡ºé”™: {e}")
    
    async def _analyze_cluster_with_retry(
        self,
        ai_service,
        emails: List[Dict[str, Any]],
        prompt_template: str,
        max_retries: int,
        task_id: str = None
    ) -> Optional[str]:
        """å¸¦é‡è¯•çš„èšç±»åˆ†æ"""
        import json as json_lib
        from services.email_dedup_service import EmailDedupService
        from services.pii_masking_service import PIIMaskingService
        
        # è·å–æˆ–åˆ›å»ºä»»åŠ¡çº§åˆ«çš„è„±æ•æœåŠ¡å®ä¾‹
        if task_id and task_id not in self.task_masking_services:
            self.task_masking_services[task_id] = PIIMaskingService()
        masking_service = self.task_masking_services.get(task_id) if task_id else PIIMaskingService()
        
        # æ„å»ºåˆ†æä¸Šä¸‹æ–‡
        raw_context = EmailDedupService.build_deduped_context(emails)
        
        # ğŸ”’ è„±æ•å¤„ç†ï¼šå°†æ•æ„Ÿä¿¡æ¯æ›¿æ¢ä¸º Token
        masked_context, token_map = masking_service.mask_text(raw_context)
        
        # è®°å½•è„±æ•ç»Ÿè®¡ï¼ˆè°ƒè¯•ç”¨ï¼‰
        if token_map:
            stats = masking_service.get_statistics()
            print(f"[PII] Cluster: è„±æ•ç»Ÿè®¡ {stats}")
        
        for attempt in range(max_retries):
            try:
                # è°ƒç”¨ AI æœåŠ¡
                # âš ï¸ å…³é”®ï¼šä½¿ç”¨è„±æ•åçš„ä¸Šä¸‹æ–‡ï¼Œç¡®ä¿æ•æ„Ÿä¿¡æ¯ä¸æ³„éœ²ç»™ LLM
                result_model = await asyncio.wait_for(
                    ai_service.analyze_email(masked_context, prompt_template),
                    timeout=90.0  # èšç±»æ–‡æœ¬è¾ƒé•¿ï¼Œç»™äºˆæ›´å¤šæ—¶é—´
                )
                
                # èšç±»åˆ†æç›®å‰æœŸæœ›è¿”å› JSON å­—ç¬¦ä¸²
                return json_lib.dumps(result_model.model_dump(), ensure_ascii=False)
                
            except asyncio.TimeoutError:
                if attempt < max_retries - 1:
                    await asyncio.sleep(2)
            except Exception as e:
                # print(f"Cluster analysis failed: {e}") # Debug log
                if attempt < max_retries - 1:
                    await asyncio.sleep((2 ** attempt))
                else:
                    return None
        
        return None


# å•æ¡é‚®ä»¶åˆ†æ
async def analyze_single_email(
    task_id: str,
    email_id: int,
    prompt: str = None,
    model: str = None
) -> Dict[str, Any]:
    """
    åˆ†æå•æ¡é‚®ä»¶
    
    Args:
        task_id: ä»»åŠ¡ ID
        email_id: é‚®ä»¶ ID
        prompt: åˆ†æ Prompt
        model: AI æ¨¡å‹
    
    Returns:
        åˆ†æç»“æœ
    """
    import json as json_lib
    
    db = get_db_service()
    
    # è·å–é‚®ä»¶
    email = db.get_email_by_id(email_id)
    if not email:
        raise ValueError("é‚®ä»¶ä¸å­˜åœ¨")
    
    if email.get("task_id") != task_id:
        raise ValueError("é‚®ä»¶ä¸å±äºè¯¥ä»»åŠ¡")
    
    # ä½¿ç”¨é»˜è®¤å€¼æˆ–ä»æœ€è¿‘çš„æ‰¹é‡ä»»åŠ¡ä¸­è·å– Prompt
    if prompt is None:
        # å°è¯•è·å–æœ€è¿‘çš„æ‰¹é‡åˆ†æä»»åŠ¡é…ç½®
        jobs = db.get_batch_jobs_by_task(task_id)
        # è¿‡æ»¤å‡ºé‚®ä»¶åˆ†æç±»å‹çš„ä»»åŠ¡
        latest_email_job = next((job for job in jobs if job.get("analysis_type", "email") == "email"), None)
        
        if latest_email_job and latest_email_job.get("prompt"):
            prompt = latest_email_job["prompt"]
        else:
            prompt = DEFAULT_ANALYSIS_PROMPT
            
    if model is None:
        model = get_config_service().get_llm_provider()
    
    # è·å– AI æœåŠ¡
    if model == "azure":
        from services.azure_service import AzureService
        ai_service = AzureService()
    else:
        from services.gemini_service import GeminiService
        ai_service = GeminiService()
    
    # æ„å»ºåˆ†ææ–‡æœ¬
    raw_text = f"ä¸»é¢˜: {email.get('subject', 'æ— ä¸»é¢˜')}\n\n{email.get('content', '')}"
    
    # ğŸ”’ è„±æ•å¤„ç†ï¼šé˜²æ­¢æ•æ„Ÿä¿¡æ¯æ³„éœ²ç»™ LLM
    from services.pii_masking_service import PIIMaskingService
    masking_service = PIIMaskingService()
    masked_text, token_map = masking_service.mask_text(raw_text)
    
    # è®°å½•è„±æ•ç»Ÿè®¡ï¼ˆè°ƒè¯•ç”¨ï¼‰
    if token_map:
        stats = masking_service.get_statistics()
        print(f"[PII] Single Email {email_id}: è„±æ•ç»Ÿè®¡ {stats}")
    
    # è°ƒç”¨ AI
    # ä½¿ç”¨ unified analyze_email
    try:
        result_model = await ai_service.analyze_email(masked_text, prompt)
        analysis_result = result_model.model_dump()
        analysis_result["analyzed_at"] = datetime.now().isoformat()
    except Exception as e:
        # Fallback for failure
        analysis_result = {
            "summary": "åˆ†æå¤±è´¥",
            "risk_level": "ä½",
            "tags": [],
            "key_findings": f"Error: {str(e)}",
            "key_points": [],
            "analyzed_at": datetime.now().isoformat()
        }
    
    # ä¿å­˜ç»“æœ
    analysis_id = str(uuid.uuid4())
    db.save_analysis_result(
        result_id=analysis_id,
        task_id=task_id,
        email_id=email_id,
        analysis_type="batch_summary",
        model_provider=model,
        result=analysis_result
    )
    
    return {
        "analysis_id": analysis_id,
        "email_id": email_id,
        "model_provider": model,
        "result": analysis_result
    }


# å…¨å±€æœåŠ¡å®ä¾‹
_batch_analysis_service: Optional[BatchAnalysisService] = None


def get_batch_analysis_service() -> BatchAnalysisService:
    """è·å–æ‰¹é‡åˆ†ææœåŠ¡å®ä¾‹ï¼ˆå•ä¾‹æ¨¡å¼ï¼‰"""
    global _batch_analysis_service
    if _batch_analysis_service is None:
        _batch_analysis_service = BatchAnalysisService()
    return _batch_analysis_service
